import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_dataset
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("âœ… device:", device)

# âœ… í•™ìŠµëœ TinyBERT DBPedia ëª¨ë¸ ê²½ë¡œ
checkpoint_path = "./tinybert-dbpedia/checkpoint-70000"  # ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ë¡œ ìˆ˜ì •

# âœ… í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("huawei-noah/TinyBERT_General_4L_312D")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)
model.to(device)
model.eval()

# âœ… DBPedia test ë°ì´í„°ì…‹ ë¡œë”©
dataset = load_dataset("dbpedia_14")
test_texts = dataset["test"]["content"]
test_labels = dataset["test"]["label"]
label_names = dataset["test"].features["label"].names

# âœ… ë°°ì¹˜ ë‹¨ìœ„ ì¶”ë¡ 
preds = []
wrong_samples = []  # ì˜¤ë‹µ ì €ì¥ìš©
batch_size = 32

for i in tqdm(range(0, len(test_texts), batch_size)):
    batch_texts = test_texts[i:i+batch_size]
    batch_labels = test_labels[i:i+batch_size]

    encodings = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=128)
    encodings = {k: v.to(device) for k, v in encodings.items()}

    with torch.no_grad():
        logits = model(**encodings).logits
        batch_preds = torch.argmax(logits, dim=1).cpu().tolist()
        preds.extend(batch_preds)

        # ì˜¤ë‹µ í™•ì¸
        for j, (pred, label) in enumerate(zip(batch_preds, batch_labels)):
            if pred != label:
                wrong_samples.append({
                    "text": batch_texts[j],
                    "true_label": label_names[label],
                    "pred_label": label_names[pred]
                })

# âœ… ì •í™•ë„ ê³„ì‚°
acc = accuracy_score(test_labels, preds)
print(f"\nâœ… TinyBERT DBPedia í‰ê·  ì •í™•ë„: {acc:.4f}")
print(f"âŒ ì˜¤ë‹µ ìˆ˜: {len(wrong_samples)}")

# âœ… ì˜¤ë‹µ ëª‡ ê°œ ì¶œë ¥ ì˜ˆì‹œ
print("\nğŸ“Œ ì˜ˆì¸¡ì— ì‹¤íŒ¨í•œ ìƒ˜í”Œ ì˜ˆì‹œ:")
for i, sample in enumerate(wrong_samples[:5]):
    print(f"\n[{i+1}]")
    print(f"ë¬¸ì¥: {sample['text']}")
    print(f"  â–¶ ì •ë‹µ: {sample['true_label']}")
    print(f"  â–¶ ì˜ˆì¸¡: {sample['pred_label']}")
