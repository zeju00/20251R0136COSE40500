# tinybert_shap_example.py

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import shap
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings("ignore")

# 1. ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 2. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
model_path = "./tinybert-agnews/checkpoint-1250"  # ì›í•˜ëŠ” checkpoint ë””ë ‰í† ë¦¬
base_tokenizer_path = "huawei-noah/TinyBERT_General_4L_312D"

tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)
model.to(device)
model.eval()

# 3. forward í•¨ìˆ˜ ì •ì˜
def forward_func(texts):
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=128)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.logits

# 4. ë¶„ì„í•  ë¬¸ì¥
texts = [
    "NASA successfully launched a new satellite.",
    "The stock market crashed after inflation data was released."
]

# 5. SHAP ë¶„ì„
explainer = shap.Explainer(forward_func, tokenizer)
shap_values = explainer(texts)

# 6. ì‹œê°í™”
for i, text in enumerate(texts):
    print(f"\nğŸ“ ë¬¸ì¥ {i+1}: {text}")
    shap.plots.text(shap_values[i], display=True)

# 7. (ì„ íƒ) ê°œë³„ í´ë˜ìŠ¤ë³„ í™•ë¥  ì¶œë ¥
print("\nğŸ”¢ Softmax í™•ë¥ :")
for i, text in enumerate(texts):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        logits = model(**inputs).logits[0]
        probs = F.softmax(logits, dim=0).cpu().numpy()
        print(f"ë¬¸ì¥ {i+1}: {probs.round(4)}")

