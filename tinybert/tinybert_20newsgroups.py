from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import Dataset, DatasetDict
import evaluate
import torch
import torch.nn.functional as F
from sklearn.datasets import fetch_20newsgroups

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("device:", device)

# TinyBERT ëª¨ë¸
model_name = "huawei-noah/TinyBERT_General_4L_312D"

# ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=20)  # âœ… 20 classes

# âœ… 20 Newsgroups ë°ì´í„°ì…‹ ë¡œë”©
# 1. ë°ì´í„° ë¡œë“œ
train_data = fetch_20newsgroups(subset="train", remove=("headers", "footers", "quotes"))
test_data = fetch_20newsgroups(subset="test", remove=("headers", "footers", "quotes"))

# 2. Hugging Face Dataset í˜•íƒœë¡œ ë³€í™˜
train_dataset = Dataset.from_dict({"text": train_data.data, "label": train_data.target})
test_dataset = Dataset.from_dict({"text": test_data.data, "label": test_data.target})
dataset = DatasetDict({"train": train_dataset, "test": test_dataset})

label_names = train_data.target_names
print(label_names)

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
def preprocess(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)

encoded = dataset.map(preprocess, batched=True)

# í•™ìŠµ/í‰ê°€ìš© ìƒ˜í”Œ ì¶•ì†Œ (ì „ì²´ ì“°ê³  ì‹¶ìœ¼ë©´ select ì œê±°)
#train_dataset = encoded["train"].select(range(4000))
#eval_dataset = encoded["test"].select(range(1000))
train_dataset = encoded["train"]
eval_dataset = encoded["test"]
# ì •í™•ë„ ì¸¡ì •
accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = torch.argmax(torch.tensor(logits), dim=1)
    return accuracy.compute(predictions=preds, references=labels)

# í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./tinybert-newsgroups",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=32,
    num_train_epochs=3,
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=100,
    report_to='none',
    no_cuda=True,
)

# Trainer ì„¤ì •
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()

# ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
text = "The government plans to increase taxes on imported cars."

inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
model.to(device)
inputs = {k: v.to(device) for k, v in inputs.items()}

model.eval()
with torch.no_grad():
    outputs = model(**inputs)
    probs = F.softmax(outputs.logits, dim=1).cpu().numpy()
    pred = torch.argmax(outputs.logits, dim=1).item()

# ë¼ë²¨ ì •ë³´ ì¶”ì¶œ (ë°ì´í„°ì…‹ì—ì„œ ì œê³µ)
label_names = train_data.target_names

print("\nğŸ§  ì˜ˆì¸¡ í´ë˜ìŠ¤:", label_names[pred])
print("\nğŸ“Š Soft labels (í™•ë¥ ):")
for i, p in enumerate(probs[0]):
    print(f"{label_names[i]}: {p:.4f}")
